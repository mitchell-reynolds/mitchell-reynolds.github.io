---
title: SERI ML Alignment Theory Scholars Program
published: true
categories:
- agi
- eudaimonia
---

I'm applying to the [SERI ML Alignment Theory Scholars [MATS]](https://www.serimats.org/program) program for the [Multipolar](https://www.serimats.org/multipolar) track under the mentorship of [Jesse Clifton](https://www.alignmentforum.org/users/jesseclifton) and [Daniel Kokotajlo](https://www.lesswrong.com/users/daniel-kokotajlo). They recommend reading their [Introduction](https://www.lesswrong.com/s/32kWH6hqFhmdFsvBh/p/oNQGoySbpmnH632bG) of their [When does technical work to reduce AGI conflect make a difference?](https://www.lesswrong.com/s/32kWH6hqFhmdFsvBh) sequence. 

# General Question 1
> What is your experience with AI Alignment? (3-5 sentences). 
> If you have any projects or posts, please provide links when able!
> This might include:
> - Completing AGI Safety Fundamentals, AI Safety Camp, MLAB, MLSS, PIBBSS, etc.
> - Reading/posting on LessWrong
> - Reading Superintelligence, The Precipice, The Alignment Problem, Human Compatible, Life 3.0, etc.

In 2021, I completed AGI Safety Fundamentals and have been invited back as a Faciliator for the program both in 2022 and in 2023. I've committed [~300 hours toward AI Safety martials](https://mitchell-reynolds.github.io/Why-AGI#ai-safety-understanding-300-hours) including reading Superintelligence, Human Compatible, all Cold Takes posts on AI, and various LessWrong & Alignment forum posts. In parallel to preparing for this Program, I'm writing a draft for Open Philanthropy's AI Worldviews contest [here](https://mitchell-reynolds.github.io/AI-Worldviews-Competition) that aims to quantify the probability of AGI existential catastrophe from humans lossing control of the system. Lastly, my explicit career focus is to work in AI Safety where I'm prioritizing manager positions with openness to individual contributor roles in the short term.


# General Question 2
> What is your previous experience with research? (3-5 sentences).
> _Research experience in fields like math/CS/ML/philosophy may play a role in your ability to do alignment theory. Feel free to refer to your LinkedIn/resume._

I have [~4000 hours of AI capabilities background](https://mitchell-reynolds.github.io/Why-AGI#ai-capabilities-understanding-4000-hours) with relevant research overlap. My Economics undergraduate senior thesis research was accepted in the 2015 Economics Scholars Program in collaboration with The Federal Reserve Bank. After working as an Analyst for a couple of years, I completed an intensive 800 hour data science coding bootcamp where my capstone research project focused on forecasting the venture captial market. In my career & in spare time, I've developed dozens of ML models ranging from linear regressions, ARIMA, Bayesian hierarchical modelling, decision trees, random forests, XGBoost, CNN, YOLO, RNN, LSTM, and GANs.


# Multipolar Problem 1
> In [“When would AGIs engage in conflict?”](https://www.lesswrong.com/posts/cLDcKgvM6KxBhqhGq/when-would-agis-engage-in-conflict), we list mechanisms that could lead to costly conflict between intelligent agents. Conditioning on a catastrophic conflict between AGI agents, which of these mechanisms do you think is the most likely explanation and why?

# Multipolar Problem 2
> Play around with generating text with text-davinci-003 and code-davinci-002 and explore the ways in which they differ. What do you see as the most important differences, and how will this affect how each might be useful for doing alignment research? Set the temperature to 1 for this exercises.

