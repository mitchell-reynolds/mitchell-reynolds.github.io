---
title: SERI ML Alignment Theory Scholars Program
published: true
categories:
- agi
- eudaimonia
---

I'm applying to the [SERI ML Alignment Theory Scholars [MATS]](https://www.serimats.org/program) program for the [Multipolar](https://www.serimats.org/multipolar) track under the mentorship of [Jesse Clifton](https://www.alignmentforum.org/users/jesseclifton) and [Daniel Kokotajlo](https://www.lesswrong.com/users/daniel-kokotajlo).

# General Question 1
> What is your experience with AI Alignment? (3-5 sentences)
> This might include:
> - Completing AGI Safety Fundamentals, AI Safety Camp, MLAB, MLSS, PIBBSS, etc.
> - Reading/posting on LessWrong
> - Reading Superintelligence, The Precipice, The Alignment Problem, Human Compatible, Life 3.0, etc.
> If you have any projects or posts, please provide links when able!



# General Question 2
> What is your previous experience with research? (3-5 sentences)
> Research experience in fields like math/CS/ML/philosophy may play a role in your ability to do alignment theory. Feel free to refer to your LinkedIn/resume.


# Multipolar Problem 1
> In [“When would AGIs engage in conflict?”](https://www.lesswrong.com/posts/cLDcKgvM6KxBhqhGq/when-would-agis-engage-in-conflict), we list mechanisms that could lead to costly conflict between intelligent agents. Conditioning on a catastrophic conflict between AGI agents, which of these mechanisms do you think is the most likely explanation and why?

# Multipolar Problem 2
> Play around with generating text with text-davinci-003 and code-davinci-002 and explore the ways in which they differ. What do you see as the most important differences, and how will this affect how each might be useful for doing alignment research? Set the temperature to 1 for this exercises.

