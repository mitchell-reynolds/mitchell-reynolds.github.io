---
title: SERI ML Alignment Theory Scholars Program
published: true
categories:
- agi
- eudaimonia
---

I'm applying to the [SERI ML Alignment Theory Scholars [MATS]](https://www.serimats.org/program) program for the [Multipolar](https://www.serimats.org/multipolar) track under the mentorship of [Jesse Clifton](https://www.alignmentforum.org/users/jesseclifton) and [Daniel Kokotajlo](https://www.lesswrong.com/users/daniel-kokotajlo). They recommend reading their [Introduction](https://www.lesswrong.com/s/32kWH6hqFhmdFsvBh/p/oNQGoySbpmnH632bG) of their [When does technical work to reduce AGI conflict make a difference?](https://www.lesswrong.com/s/32kWH6hqFhmdFsvBh) sequence. 

# General Question 1
> What is your experience with AI Alignment? (3-5 sentences). 
> If you have any projects or posts, please provide links when able!
> This might include:
> - Completing AGI Safety Fundamentals, AI Safety Camp, MLAB, MLSS, PIBBSS, etc.
> - Reading/posting on LessWrong
> - Reading Superintelligence, The Precipice, The Alignment Problem, Human Compatible, Life 3.0, etc.

In 2021, I completed AGI Safety Fundamentals and have been invited back as a Facilitator for the program both in 2022 and in 2023. I've committed [~300 hours toward AI Safety martial](https://mitchell-reynolds.github.io/Why-AGI#ai-safety-understanding-300-hours) including reading Superintelligence, Human Compatible, all Cold Takes posts on AI, and various LessWrong & Alignment Forum posts. In parallel to preparing for this Program, I'm writing a forecasting paper for Open Philanthropy's AI Worldviews contest [here](https://mitchell-reynolds.github.io/AI-Worldviews-Competition) that aims to quantify the probability of AGI existential catastrophe from humans losing control. Lastly, my explicit career focus is to work in AI Safety where I'm prioritizing manager positions with openness to individual contributor roles in the short term.


# General Question 2
> What is your previous experience with research? (3-5 sentences).

> _Research experience in fields like math/CS/ML/philosophy may play a role in your ability to do alignment theory. Feel free to refer to your LinkedIn/resume._

I have [~4000 hours of AI capabilities background](https://mitchell-reynolds.github.io/Why-AGI#ai-capabilities-understanding-4000-hours) with relevant research overlap. I presented my economics undergraduate senior thesis in the 2015 Economics Scholars Program in collaboration with The Federal Reserve Bank. After working as an Analyst for a couple of years, I completed an intensive 800 hour data science coding bootcamp where my capstone research project focused on forecasting the venture capital market. In my career & in my spare time via online courses/Meetups/Kaggle, I've developed dozens of ML models with the following list of descending order in experience: linear regression, ARIMA, RNN, LSTM, decision trees, random forests, GANs, XGBoost, CNN, YOLO, Bayesian hierarchical modeling, and transformers.


# Multipolar Problem 1 (~250-500 words)
> In [“When would AGIs engage in conflict?”](https://www.alignmentforum.org/posts/cLDcKgvM6KxBhqhGq/when-would-agis-engage-in-conflict), we list mechanisms that could lead to costly conflict between intelligent agents. Conditioning on a catastrophic conflict between AGI agents, which of these mechanisms do you think is the most likely explanation and why?

# Multipolar Problem 2 (~250-500 words)
> Name a disposition that an AI system might have that would significantly increase or reduce its losses from conflict, or the losses it inflicts on other agents. (For example, a spiteful agent might be particularly prone to conflict, while an agent with a [surrogate goal](https://longtermrisk.org/spi) might be less likely to face large losses from bargaining failure.) Explain your answer. Then, assume a particular alignment strategy and sketch how we might reduce or increase the chances of an AI having that disposition under the assumed alignment strategy.

