---
title: AGI Safety Fundamentals Summary - Part 1
published: true
categories:
- agi
---

# Background, Context, & Goals
This is the first installment of my summary for the Artificial General Intelligence Safety Fundamentals Program [Program] 
offered by 
[BlueDot,](https://www.agisafetyfundamentals.com/) which spun out of 
[Effective Altruism Cambridge](https://www.eacambridge.org/) in collaboration with
[Richard Ngo.](https://www.richardcngo.com/)
My involvement with the Program began in 2021 where I started as a participant facilitated by Michael Chen. 
In 2022 and 2023, I was offered to be a paid facilitator for 1 cohort each iteration. 

My goals for facilitating are to:
1. Improve participants understanding on this important topic.
2. Establish connections with those also interested in contributing to the AI alignment problem.
   - Connections (Direct work): Either these folks will be future colleagues or will have overlap in.
   - Connections (Indirect work): As most participants are technical, I can see a future where SWE/researchers are working for any of the [MAMAA](https://fortune.com/2021/10/29/faang-mamaa-jim-cramer-tech-facebook-meta/) companies and could be part of the capabilities frontier.
3. Improve my clarity and communication around this challenging topic while also staying up to date on the latest research.
4. [Bonus] Establish myself as a credible, nuanced source of AGI safety knowledge.
5. [Bonus] Directly contribute to the advancement of ["helpful, honest, and harmless"](https://ar5iv.labs.arxiv.org/html/2112.00861) AI systems with a focus on strategy.

# Week 0: Introduction to ML

# Week 1: Artificial General Intelligence

# Week 2: Reward misspecification and instrumental convergence

# Week 3: Goal misgeneralization

# Links to Materials [~7 hours]

_I made a [Spotify Playlist](https://open.spotify.com/playlist/4RV5q7Z49XZflV38NoahF5?si=2567ed53d3944784) for the 
2022 iteration of the Program with **some** of the readings that should be viewed as supplemental and not as a substitution._ 

## Materials for Week 0: Introduction to ML [~165 minutes]
### Readings
- [A short introduction to machine learning by Richard Ngo (2021)](https://www.alignmentforum.org/posts/qE73pqxAZmeACsAdF/a-short-introduction-to-machine-learning)
- [Machine Learning for Humans (2017) by Vishi Maini and Sabri](https://medium.com/@v_maini/supervised-learning-740383a2feab)

### Videos
- [But what is a neural network(2017)](https://www.youtube.com/watch?v=aircAruvnKk)
- [Gradient descent, how neural networks learn (2017)](https://www.youtube.com/watch?v=IHZwWFHWa-w)
- [What is backpropagation really doing? (2021)](https://www.youtube.com/watch?v=Ilg3gGewQ5U)
- [What is self-supervised learning? (2021)](https://youtu.be/sJzuNAisXHA)
- [Introduction to reinforcement learning (2021)](https://www.youtube.com/watch?v=TCCjZe0y4Qc&list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm)
- [Transformers, explained: Understand the model behind GPT, BERT, and T5 (2021)](https://www.youtube.com/watch?v=SZorAJ4I-sA)

---

## Materials for Week 1: Artificial General Intelligence [~95 minutes]
- [Visualizing the deep learning revolution (2023) by Richard Ngo](https://medium.com/@richardcngo/visualizing-the-deep-learning-revolution-722098eb9c5)
- [On the opportunities and risks of foundation models (2022) by Bommasani et alia](https://arxiv.org/pdf/2108.07258.pdf)
- [Four Background Claims (2015) by Nate Soares](https://intelligence.org/2015/07/24/four-background-claims/)
- [AGI safety from first principles (2020) by Richard Ngo](https://drive.google.com/file/d/1uK7NhdSKprQKZnRjU58X7NLA1auXlWHt/view)
- [[Video] Why and how of scaling large language models (2022) by Nicholas Joseph](https://www.youtube.com/watch?v=qscouq3lo0s)
- [Biological Anchors: A Trick That Might Or Might Not Work (2022) by Scott Alexander](https://astralcodexten.substack.com/p/biological-anchors-a-trick-that-might)
- [Intelligence explosion: evidence and import (2012) by Luke Muehlhauser and Anna Salamon](https://drive.google.com/file/d/1QxMuScnYvyq-XmxYeqBRHKz7cZoOosHr/view?usp=sharing)
- [[Advanced ML] Future ML Systems Will Be Qualitatively Different (2022) by Jacob Steinhardt](https://bounded-regret.ghost.io/future-ml-systems-will-be-qualitatively-different/)
- [[Advanced ML] More Is Different for AI (2022) by Jacob Steinhardt](https://bounded-regret.ghost.io/more-is-different-for-ai/)

---

## Materials for Week 2: Reward misspecification and instrumental convergence [~80 minutes]
- [Specification gaming: the flip side of AI ingenuity (2020) by Victoria Krakovna et alia](https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity)
- [Learning from Human Preferences (2017) by Paul Christiano, Alex Ray and Dario Amodei](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/)
- [Learning to Summarize with Human Feedback (2020) by Jeffrey Wu et alia](https://openai.com/blog/learning-to-summarize-with-human-feedback/)
- [The alignment problem from a deep learning perspective (2022) by Richard Ngo et alia](https://ar5iv.labs.arxiv.org/html/2209.00626)
- [Superintelligence: Instrumental convergence (2014) by Nick Bostrom](https://drive.google.com/file/d/1KewDov1taegTzrqJ4uurmJ2CJ0Y72EU3/view?usp=sharing)
- [[Video]Inverse reinforcement learning example (2016)](https://www.youtube.com/watch?v=h7uGyBcIeII)
- [The easy goal inference problem is still hard (2018) by Paul Christiano](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/h9DesGT3WT9u2k7Hr)
- [[Advanced ML] Optimal Policies Tend To Seek Power (2021) by Alex Turner et alia](https://neurips.cc/virtual/2021/poster/28400)

---

## Materials for Week 3: Goal misgeneralization [~95 minutes]
- [Goal misgeneralization: why correct specifications arenâ€™t enough for correct goals (2022) by Rohin Shah et alia](https://ar5iv.labs.arxiv.org/html/2210.01790)
- [[Video]The other alignment problem: mesa-optimisers and inner alignment (2021)](https://youtu.be/bJLcIBixGj8)
- [Why alignment could be hard with modern deep learning (2021) by Ajeya Cotra](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/)
- [What failure looks like (2019) by Paul Christiano](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like)
- [[Advanced ML] Thought Experiments Provide a Third Anchor (2022) by Jacob Steinhardt](https://bounded-regret.ghost.io/thought-experiments-provide-a-third-anchor/)
- [[Advanced ML] ML Systems Will Have Weird Failure Modes (2022) by Jacob Steinhardt](https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/)
- [[Advanced ML]The alignment problem from a deep learning perspective (2022) by Richard Ngo et alia](https://ar5iv.labs.arxiv.org/html/2209.00626)