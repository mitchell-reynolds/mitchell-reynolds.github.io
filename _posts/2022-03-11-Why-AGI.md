---
title: Why AGI?
published: true
categories:
- agi
---
_Updated Feb 2023_

In a university philosophy course, I read a
[paper](https://vtechworks.lib.vt.edu/bitstream/handle/10919/89424/TechReport05-3.pdf?sequence=1) 
that first touched on the topic of humankind's last invention. 
At that time I was fascinated by the concept of an Artificial General Intelligence [AGI]
that would self improve itself and do so at a near-exponential rate. 
I'd later learn this is called a "fast take off" and might _seem_ broadly beneficial to humanity prima facie. 
An optimist might first think that an AGI could readily improve our world say by curing cancer, ending wars, 
and other noncontroversial stances.

However upon closer inspection, I've come to hold the belief that safe AGI is not guaranteed. 
Therefore, I want to explicitly outline why I believe that AGI poses a unique threat in my own words. 
I'm writing this to be "by-and-large" true over precisely true.

# Beliefs & Arguments
1. **Orthongonality Thesis**: Goals and Intelligence are two different axises within an AGI. Therefore, the space of possibilities includes "Superintelligence, Misaligned with Human Goals."
2. An AGI being misaligned to human values seems more likely than to be aligned. This is because by-and-large the AGI will seek power (almost always an instrumental goal) to accomplish its terminal or final goal.
3. Even if the above two are solved, it is still possible for an AGI to create a terrible outcome that results from the **value loading problem.**
    - Using the example goal of "cure cancer" to an AGI: the AGI _could_ solve for this by giving everyone a pill where they die at the age of 40. The AGI found that cancer is much more common in old age and solved the goal given but is not aligned with human values.

# Current Background Knowledge
I've thoughtfully engaged in learning about AGI Risk in the hundreds (not yet thousands) of hours and am not an expert yet. 
My goal is to place my thinking online for further refinement & clarity. 
Professionally, I have seen first hand the strengths, the misunderstanding, & the misuse of narrow artificial intelligence systems.

### AGI Risk Background [100s of hours]
- [AGI Safety Fundamentals Program - Technical Alignment](https://www.eacambridge.org/technical-alignment-curriculum) as a Participant then as a Facilitator multiple (estimated reading time = 60 hours; 40 hours of discussion).
- **Superintelligence** by Nick Bostrom (estimated reading time = 10 hours).
- **Human Compatible** by Stuart J. Russell (reading now and estimated reading time = 5 hours).
- [Cold Takes Blog](https://cold-takes.com/) by Holden Karnofsky (AI ~5 hours but read & really enjoy all his posts).
- Read several AI newsletters with a focus on safety/alignment (est. 2 hours per week).
     - [Import AI](https://jack-clark.net/) & [Last Week in AI](https://lastweekin.ai/)
     - [Alignment Newsletter](http://rohinshah.com/alignment-newsletter/) & [ML Safety Newsletter](https://newsletter.mlsafety.org/)
     - [The Batch by Andrew Ng](https://www.deeplearning.ai/the-batch/)

### AI Training [1000s of hours]
- Deployed half a dozen ML models at various companies [1000s of hours since 2017].
- Deep Learning Specialization [150 hours].
- Multiple meetups with hands on tutorials with deep learning, NLP, & computer vision [est. 10-20 hours].
- Kaggle Competitions [est. 20-40 hours].
- Data Science Bootcamp covering computer science and mathematics [500 hours].
- University Courses (Econometrics; Statistics; Calculus) [700 hours].

# Where to go from here?
"What's the world's most pressing problem and how are you using your career to solve it?" -Hamming Question

In my view, the AI Alignment problem is the world's most pressing problem. 
I'm optimistic humanity can solve the above introductory problems & the many other more nuanced problems 
I've discovered along the way. 
My [2015-self wrote](./Reaching-The-Summit) about self-driving cars & aligning my career toward this narrow AI direction. 
I moved closer toward narrow AI by [completing a data science bootcamp in 2017](./Bootcamp-And-Beyond). 
[Connecting the dots along the way](https://youtu.be/UF8uR6Z6KLc), my next career step or shortly thereafter (<5 years) will be to directly contribute to AI Safety. 
Given [my experience so far](./All-Jobs-to-Date), I believe a technical management has the highest degree of fit.

My strategy is to be the [Pareto Best in the world](https://www.lesswrong.com/posts/XvN2QQpKTuEzgkZHY/being-the-pareto-best-in-the-world) in some combination that will assist in alignment strategy work.
I'm actively searching this vast space of what combinations make sense for me to categorize me as being truly world-class.
