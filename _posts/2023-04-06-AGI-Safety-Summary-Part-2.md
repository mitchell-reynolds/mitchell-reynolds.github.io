---
title: AGI Safety Fundamentals Summary (2023 Cohort) - Part 2
published: false
categories:
- agi
---

# Background, Context, & Goals
This is the first installment of my summary & extrapolations for the 
Artificial General Intelligence Safety Fundamentals Program [Program] offered by 
[BlueDot,](https://www.agisafetyfundamentals.com/) which spun out of 
[Effective Altruism Cambridge](https://www.eacambridge.org/) in collaboration with
[Richard Ngo](https://www.richardcngo.com/).
Although I have [~1000s hours of AI/ML training](https://mitchell-reynolds.github.io/Why-AGI#ai-training-1000s-of-hours),
the program begins with an optional Week 0 that takes ~2 hours to get up to speed conceptually. 
I will assume this (admittedly small) audience has this level of familiarity and 
that these summaries go more into "what it's like inside my mind." 
Lastly, my involvement with the Program began in 2021 where I started as a participant facilitated by Michael Chen. 
In both 2022 & 2023, I was offered to be a paid facilitator for 1 cohort each iteration. 

## Instrumental Goals
My instrumental goals for facilitating the Program & independently writing these summaries are to:
1. Improve participants' & readers' understanding on this important topic.
2. Establish connections with those also interested in contributing to the AI alignment problem.
   - Connections (Direct work): Either these folks will be future colleagues or will have overlap in the field of AI alignment. In my view, I believe most industries will be impacted by the advancements of AI.
   - Connections (Indirect work): As most participants are technical, I can see a future where SWE/researchers are working for any of the [MAMAA](https://fortune.com/2021/10/29/faang-mamaa-jim-cramer-tech-facebook-meta/) companies and could be part of the capabilities frontier. I believe that capabilities SWE/researchers should be knowledgeable about AGI safety.
3. Improve, clarify, & solidify my own understanding around this challenging topic while also staying up to date on the latest research.
4. Practice my oral & written communication skills for AGI safety information and to ["find ways to help people understand the core parts of the challenges we might face, in as much detail as is feasible."](https://www.cold-takes.com/spreading-messages-to-help-with-the-most-important-century/)
5. [Bonus] Establish myself as a credible & properly nuanced source of AGI safety knowledge.
6. [Bonus] By staying up-to-date on the latest research, directly contribute to the advancement of ["helpful, honest, and harmless"](https://ar5iv.labs.arxiv.org/html/2112.00861) AI systems with a focus on strategy.

My final goals are [here.](./about#purpose-lifelong)

# Week 4: Task decomposition for scalable oversight

**Scalable Oversight:** The problem of supervising systems that potentially outperform us
on most skills relevant to the task at hand.

# Week 5: Adversarial techniques for scalable oversight

# Week 6

# Week 7

# Links to Part 2 Materials

_I made a [Spotify Playlist](https://open.spotify.com/playlist/4RV5q7Z49XZflV38NoahF5?si=2567ed53d3944784) for the 
2022 iteration of the Program with **some** of the readings that should be viewed as supplemental and not as a substitution.
Lastly, I add in optional readings that I personally view as important enough to be included here & would recommend._ 

## Week 4: Task decomposition for scalable oversight
- [[Video] AI Alignment Landscape (2020) by Paul Christiano](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment)
- [Measuring Progress on Scalable Oversight for Large Language Models (2022) by Samuel Bowman](https://ar5iv.labs.arxiv.org/html/2211.03540)
- [Learning Complex Goals with Iterated Amplification (2018) by Paul Christiano and Dario Amodei](https://openai.com/blog/amplifying-ai-training/)
- [Supervising strong learners by amplifying weak experts (2018) by Paul Christiano, Dario Amodei and Buck Shlegeris](https://ar5iv.labs.arxiv.org/html/1810.08575)
- [Summarizing Books with Human Feedback (2021) by Jeffrey Wu, Ryan Lowe and Jan Leike](https://openai.com/research/summarizing-books)
- [Language Models Perform Reasoning via Chain of Thought (2022) by Jason Wei, Denny Zhou and Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)
- [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models (2022) by Denny Zhou et alia](https://ar5iv.labs.arxiv.org/html/2205.10625)

## Week 5: Adversarial techniques for scalable oversight
- [AI-written critiques help humans notice flaws: blog post (2022) by Jan Leike et alia](https://openai.com/research/critiques)
- [AI safety via debate (2018) by Geoffrey Irving, Paul Christiano and Dario Amodei](https://ar5iv.labs.arxiv.org/html/1805.00899)
- [Red-teaming language models with language models (2022) by Ethan Perez et alia](https://www.deepmind.com/blog/red-teaming-language-models-with-language-models)
- [[Advanced ML] Robust Feature-Level Adversaries are Interpretability Tools (2021) by Casper et alia](https://ar5iv.labs.arxiv.org/html/2110.03605)
- [High-stakes alignment via adversarial training blog posts (2022) by Daniel Ziegler et alia](https://www.alignmentforum.org/posts/A9tJFJY7DsGTFKKkh/high-stakes-alignment-via-adversarial-training-redwood)
- [Takeaways from our robust injury classifier project (2022) by Daniel Ziegler et alia](https://www.alignmentforum.org/posts/n3LAgnHg6ashQK3fF/takeaways-from-our-robust-injury-classifier-project-redwood)
- [[Optional] WebGPT (2021) by Jacob Hilton et alia](https://openai.com/research/webgpt)
- [[Optional] Debate update: Obfuscated arguments problem (2020) by "Beth Barnes" and Paul Christiano](https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem)
- [[Optional] Training robust corrigibility (2019) by Paul Christiano](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d)

## Week 6
- 

## Week 7
- 