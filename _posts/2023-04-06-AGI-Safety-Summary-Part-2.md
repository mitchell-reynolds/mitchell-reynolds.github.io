---
title: AGI Safety Summary (2023 Cohort) - Part 2
published: true
categories:
- agi
---

# Background, Context, & Goals üìö
This is the second installment of my summary & extrapolations for the 
Artificial General Intelligence Safety Fundamentals Alignment Course ("Course" hereafter).
Please read [Part 1](./AGI-Safety-Summary-Part-1) first as the Course builds upon itself.

## Instrumental Goals
My instrumental goals for facilitating the Course & independently writing these summaries are to:
1. Distill ~30 hours of readings & discussions into ~30 minutes of reading here.
   - 30 minute estimate = ~6k words at [200 WPM](./Human-Info-Speed#reading--speaking-speeds)
2. Improve, clarify, & solidify my own understanding around this challenging topic while also staying up to date on the latest research.
3. Practice my communication skills for AI safety information and to "find ways to help people understand the core parts of the challenges we might face, in as much detail as is feasible." [[Cold Takes]](https://www.cold-takes.com/spreading-messages-to-help-with-the-most-important-century/)
4. Establish connections with those also interested in working in AI safety.
   - Connections (Direct work): Either these folks will be future colleagues or will have overlap in the field of AI safety. In my view, I believe most industries will be impacted by the advancements of AI.
   - Connections (Indirect work): As most participants are technical, I can see a future where SWE/researchers are working for any of the [MAMAA](https://fortune.com/2021/10/29/faang-mamaa-jim-cramer-tech-facebook-meta/) companies and could be part of the capabilities frontier. I believe that capabilities SWE/researchers should be knowledgeable about AI safety.
5. [Bonus] Establish myself as a credible & properly nuanced source of AI safety knowledge.
6. [Bonus] By staying up-to-date on the latest research, directly contribute to the advancement of ["helpful, honest, and harmless"](https://ar5iv.labs.arxiv.org/html/2112.00861) AI systems with a focus on strategy.

_My final goals are [here.](./about#purpose-lifelong) Lastly, I would like to thank Greg Tracy & my cat Bunny for feedback where all the mistakes are my own or Bunny's suggestions from walking on the keyboard._üêà

# Week 4: Task decomposition for scalable oversight üëÄ

- **Scalable Oversight:** The problem of supervising systems that potentially outperform us on most skills relevant to the task at hand.
- **Task decomposition:** 
- **Iterated amplification:**

<img src="/assets/2021-christiano-alignment-map.jpeg" alt="ChristianoMap" width="1024" class="center"/>

# Week 5: Adversarial techniques for scalable oversight üëæ 

# Week 6: Interpretability üîç

# Week 7: Agent Foundations & Governance üèõÔ∏è
_My cohort chose Agent Foundations while I preferred the default choice of Governance/Strategy._

## Agent Foundations

## Governance 
[When AGI is a 'Manhattan Project' away](https://arxiv.org/ftp/arxiv/papers/2008/2008.04701.pdf)
[Anthropics LLM Surprise](https://arxiv.org/abs/2202.07785)
[Holden's AI Strategy set of questions](https://forum.effectivealtruism.org/posts/zGiD94SHwQ9MwPyfW/#Questions_about_AI_strategy__more_)

# Links to Part 2 Materials [~7 hours] üîó

_I made a [Spotify Playlist](https://open.spotify.com/playlist/4RV5q7Z49XZflV38NoahF5?si=2567ed53d3944784) for the 
2022 iteration of the Program with **some** of the readings that should be viewed as supplemental and not as a substitution.
Lastly, I add in optional readings that I personally view as important enough to be included here & would recommend._ 

## Materials for Week 4: Task decomposition for scalable oversight [~110 minutes]
- [[Video] AI Alignment Landscape (2020) by Paul Christiano](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment)
- [Measuring Progress on Scalable Oversight for Large Language Models (2022) by Samuel Bowman](https://ar5iv.labs.arxiv.org/html/2211.03540)
- [Learning Complex Goals with Iterated Amplification (2018) by Paul Christiano and Dario Amodei](https://openai.com/blog/amplifying-ai-training/)
- [Supervising strong learners by amplifying weak experts (2018) by Paul Christiano, Dario Amodei and Buck Shlegeris](https://ar5iv.labs.arxiv.org/html/1810.08575)
- [Summarizing Books with Human Feedback (2021) by Jeffrey Wu, Ryan Lowe and Jan Leike](https://openai.com/research/summarizing-books)
- [Language Models Perform Reasoning via Chain of Thought (2022) by Jason Wei, Denny Zhou and Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)
- [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models (2022) by Denny Zhou et alia](https://ar5iv.labs.arxiv.org/html/2205.10625)

## Materials for Week 5: Adversarial techniques for scalable oversight [~90 minutes]
- [AI-written critiques help humans notice flaws: blog post (2022) by Jan Leike et alia](https://openai.com/research/critiques)
- [AI safety via debate (2018) by Geoffrey Irving, Paul Christiano and Dario Amodei](https://ar5iv.labs.arxiv.org/html/1805.00899)
- [Red-teaming language models with language models (2022) by Ethan Perez et alia](https://www.deepmind.com/blog/red-teaming-language-models-with-language-models)
- [[Advanced ML] Robust Feature-Level Adversaries are Interpretability Tools (2021) by Casper et alia](https://ar5iv.labs.arxiv.org/html/2110.03605)
- [High-stakes alignment via adversarial training blog posts (2022) by Daniel Ziegler et alia](https://www.alignmentforum.org/posts/A9tJFJY7DsGTFKKkh/high-stakes-alignment-via-adversarial-training-redwood)
- [Takeaways from our robust injury classifier project (2022) by Daniel Ziegler et alia](https://www.alignmentforum.org/posts/n3LAgnHg6ashQK3fF/takeaways-from-our-robust-injury-classifier-project-redwood)
- [[Optional] WebGPT (2021) by Jacob Hilton et alia](https://openai.com/research/webgpt)
- [[Optional] Debate update: Obfuscated arguments problem (2020) by "Beth Barnes" and Paul Christiano](https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem)
- [[Optional] Training robust corrigibility (2019) by Paul Christiano](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d)

## Materials for Week 6: Interpretability [~100 minutes]
- [Feature Visualization (2017) by Chris Olah et alia](https://distill.pub/2017/feature-visualization/)
- [Zoom In: An Introduction to Circuits (2020) by Chris Olah et alia](https://distill.pub/2020/circuits/zoom-in/)
- [[Advanced ML] Toy Models of Superposition (2022) by Nelson Elhage et alia](https://transformer-circuits.pub/2022/toy_model/index.html)
- [Understanding intermediate layers using linear classifier probes (2016) by Guillaume Alain and Yoshua Bengio](https://ar5iv.labs.arxiv.org/html/1610.01644)
- [[Advanced ML] Discovering Latent Knowledge in Language Models Without Supervision (2022) by Collin Burns](https://ar5iv.labs.arxiv.org/html/2212.03827)
- [Acquisition of Chess Knowledge in AlphaZero (2021) by Thomas McGrath et alia](https://ar5iv.labs.arxiv.org/html/2111.09259)
- [Locating and Editing Factual Associations in GPT (2022) by Kevin Meng et alia](https://rome.baulab.info/)
- [[Advanced ML & Optional] Eliciting latent knowledge (2021) by Paul Christiano et alia](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit)
- [[My Addition] Chris Olah Interview on the 80k Podcast](https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/)

## Materials for Week 7 - Default: Governance [~80 minutes]
- [[Video] AI Strategy, Policy, and Governance (2019) by Allan Dafoe](https://www.youtube.com/watch?v=2IpJ8TIKKtI)
- [AI Governance: Opportunity and Theory of Impact (2020) by Allan Dafoe](https://www.governance.ai/research-paper/ai-governance-opportunity-and-theory-of-impact)
- [The AI deployment problem (2022) by Holden Karnofsky](https://www.cold-takes.com/racing-through-a-minefield-the-ai-deployment-problem/)
- [Why and How Governments Should Monitor AI Development (2021) by Jess Whittlestone and Jack Clarke](https://ar5iv.labs.arxiv.org/html/2108.12427)
- [Transformative AI and Compute (2021) by Lennart Heim](https://www.alignmentforum.org/s/bJi3hd8E8qjBeHz9Z)
- [Information security considerations for AI and the long term future (2022) by Jeffrey Ladish and Lennart Heim](https://forum.effectivealtruism.org/posts/WqQDCCLWbYfFRwubf/information-security-considerations-for-ai-and-the-long-term)
- [[Optional] Sharing Powerful AI Models (2022) by Toby Shevlane](https://www.governance.ai/post/sharing-powerful-ai-models)
- [[Optional] AI Governance: A research agenda (2018) by Allan Dafoe](https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf)
- [[Optional] Some AI governance research ideas (2021) by Markus Anderljung and Alexis Carlier](https://docs.google.com/document/d/13LJhP3ksrcEBKxYFG5GkJaC2UoxHKUYAHCRdRlpePEc/edit)
- [[Optional] The Semiconductor Supply Chain (2021) by Saif M. Khan](https://cset.georgetown.edu/publication/the-semiconductor-supply-chain/)
- [[Optional] The Global AI talent tracker (2020) by Macro Polo](https://macropolo.org/digital-projects/the-global-ai-talent-tracker/)
- [[My Addition] Could Advanced AI Drive Explosive Economic Growth (2021) by Tom Davidson](https://www.openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/)
- [GPTs are GPTs by Tyna Eloundou et alia](https://arxiv.org/pdf/2303.10130.pdf)
- [Sparks of AGI in GPT4 by Sebastien Bubeck et alia](https://arxiv.org/pdf/2303.12712.pdf)

## Materials for Week 7 - Alternative: Agent Foundations [~100 minutes]
- [What is AIXI? (2020) by Marcus Hutter](https://www.youtube.com/watch?v=g4M7stjzR1I)
- [Embedded Agents: Part 1 (2018) by Scott Garrabrant and Demski](https://intelligence.org/2018/10/29/embedded-agents/)
- [Logical decision theory (2017) by Eliezer Yudkowsky](https://arbital.com/p/logical_dt/?l=5d6)
- [Logical Induction: Blog post (2016) by Nate Soares](https://intelligence.org/2016/09/12/new-paper-logical-induction/)
- [Progress on Causal Influence diagrams: blog post (2021) by Tom Everitt et alia](https://www.alignmentforum.org/posts/Cd7Hw492RqooYgQAS/progress-on-causal-influence-diagrams)
- [Avoiding Side Effects By Considering Future Tasks (2020) by Victoria Krakovna](https://arxiv.org/abs/2010.07877)
- [Cooperation, Conflict and Transformative AI (2019) by Jesse Clifton](https://www.alignmentforum.org/s/p947tK8CoBbdpPtyK/p/KMocAf9jnAKc2jXri)