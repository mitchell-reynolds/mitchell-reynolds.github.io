---
title: AGI Safety Fundamentals Summary - Part 2
published: false
categories:
- agi
---
# Background, Context, & Goals
This is the second installment of my summary for the Artificial General Intelligence Safety Fundamentals Program [Program] 
offered by 
[BlueDot,](https://www.agisafetyfundamentals.com/) which spun out of 
[Effective Altruism Cambridge](https://www.eacambridge.org/) in collaboration with
[Richard Ngo.](https://www.richardcngo.com/)
My involvement with the Program began in 2021 where I started as a participant facilitated by Michael Chen. 
In 2022 and 2023, I was offered to be a paid facilitator for 1 cohort each iteration. 

My goals for facilitating are to:
1. Improve participants understanding on this important topic.
2. Establish connections with those also interested in contributing to the AI alignment problem.
   - Connections (Direct work): Either these folks will be future colleagues or will have overlap in.
   - Connections (Indirect work): As most participants are technical, I can see a future where SWE/researchers are working for any of the [MAMAA](https://fortune.com/2021/10/29/faang-mamaa-jim-cramer-tech-facebook-meta/) companies and could be part of the capabilities frontier.
3. Improve my clarity and communication around this challenging topic while also staying up to date on the latest research.
4. [Bonus] Establish myself as a credible, nuanced source of AGI safety knowledge.
5. [Bonus] Directly contribute to the advancement of ["helpful, honest, and harmless"](https://ar5iv.labs.arxiv.org/html/2112.00861) AI systems with a focus on strategy.

# Week 4

# Week 5

# Week 6

# Week 7

# Links to Materials

## Week 4
- 

## Week 5
- 

## Week 6
- 

## Week 7
- 