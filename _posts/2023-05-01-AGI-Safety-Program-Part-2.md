---
title: AGI Safety Fundamentals Summary - Part 2
published: false
categories:
- agi
---
# Background, Context, & Goals
This is the second installment of my summary & extrapolations for the Artificial General Intelligence Safety Fundamentals Program [Program] 
offered by 
[BlueDot,](https://www.agisafetyfundamentals.com/) which spun out of 
[Effective Altruism Cambridge](https://www.eacambridge.org/) in collaboration with
[Richard Ngo.](https://www.richardcngo.com/)
My involvement with the Program began in 2021 where I started as a participant facilitated by Michael Chen. 
In 2022 and 2023, I was offered to be a paid facilitator for 1 cohort each iteration. 

My goals for facilitating & writing these summaries are to:
1. Improve participants understanding on this important topic.
2. Establish connections with those also interested in contributing to the AI alignment problem.
   - Connections (Direct work): Either these folks will be future colleagues or will have overlap in the field of AI alignment. In my view, I most industries will be impacted by the advancements of AI.
   - Connections (Indirect work): As most participants are technical, I can see a future where SWE/researchers are working for any of the [MAMAA](https://fortune.com/2021/10/29/faang-mamaa-jim-cramer-tech-facebook-meta/) companies and could be part of the capabilities frontier. I believe that capabilities SWE/researchers should be knowledgeable about AGI safety.
3. Improve, clarify, & solidify my own understanding around this challenging topic while also staying up to date on the latest research.
4. Practice my oral & written communication skills for AGI safety information.
5. [Bonus] Establish myself as a credible, nuanced source of AGI safety knowledge.
6. [Bonus] By staying up-to-date on the latest research, directly contribute to the advancement of ["helpful, honest, and harmless"](https://ar5iv.labs.arxiv.org/html/2112.00861) AI systems with a focus on strategy.
# Week 4

# Week 5

# Week 6

# Week 7

# Links to Materials

## Week 4
- [[Video] AI Alignment Landscape (2020) by Paul Christiano](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment)
- [Measuring Progress on Scalable Oversight for Large Language Models (2022) by Samuel Bowman](https://ar5iv.labs.arxiv.org/html/2211.03540)
- [Learning Complex Goals with Iterated Amplification (2018) by Paul Christiano and Dario Amodei](https://openai.com/blog/amplifying-ai-training/)
- [Supervising strong learners by amplifying weak experts (2018) by Paul Christiano, Dario Amodei and Buck Shlegeris](https://ar5iv.labs.arxiv.org/html/1810.08575)
- [Summarizing Books with Human Feedback (2021) by Jeffrey Wu, Ryan Lowe and Jan Leike](https://openai.com/research/summarizing-books)
- [Language Models Perform Reasoning via Chain of Thought (2022) by Jason Wei, Denny Zhou and Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)
- [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models (2022) by Denny Zhou et alia](https://ar5iv.labs.arxiv.org/html/2205.10625)

## Week 5
- 

## Week 6
- 

## Week 7
- 