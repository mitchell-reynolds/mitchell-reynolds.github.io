---
title: AI Worldviews Contest
published: true
categories:
- agi
---

## Executive Summary

TODO

## Introduction & Problem Statement

This research project will focus on the second question posed in Open Philanthropy's AI Worldviews [contest](https://www.openphilanthropy.org/open-philanthropy-ai-worldviews-contest/). The question states:

> Conditional on AGI being developed by 2070, what is the probability that humanity will suffer an existential catastrophe due to loss of control over an AGI system?

Firstly, I will break down the statement precisely and define the relevant terms. The contest recognizes the thorniness of Artificial General Intelligence with a footnote emphasizing their intention to be an "AI that can quickly and affordably be trained to perform nearly all economically and strategically valuable tasks at roughly human cost or less." [TODO AGI/Human level AI; TAI/PASTA; Superintelligence].

Secondly, an existential catastrophe is an event that destroys humanity's long-term potential with the typology of human extinction, civilizational collapse, or dystopia [[EA Forum](https://forum.effectivealtruism.org/topics/existential-catastrophe-1)]. Therefore, my analysis will be the sum of these three possible types.
[Michael Aird](https://forum.effectivealtruism.org/posts/AJbZ2hHR4bmeZKznG/venn-diagrams-of-existential-global-and-suffering) has the following helpful diagram from Toby Ord's work:

<img src="/assets/2020-ai-worldviews-aird-catastrophe-tree.png" alt="CatastropheTree" width="1024" class="center"/>

Lastly for definitions, I will define the two salient timeframes where one is explicit and the other is implicit. The first explicit timing is "by 2070" means that AGI can be developed between now 2023 and by 2070. Next, the question has "will suffer" and when combined with AGI development to be anywhere between 2023 and to be potentially ever. In order to focus my research, I will limit my analysis to be the end of this century.

Taking these points together reduces the space of possibilities. 
Therefore, this analysis will ignore the scenario where a superintelligent agent allows us to pursue our own goals because it has the rest of the universe to conquer for its own ends. This second case isn't one of the three types, but it seems to fall within the scope of "destorying humanity's long-term potential." Given that my analysis focuses on this century alone, it is unclear we'd accumulate an astronomical portion of the gains from our potential cosmic endowment yet recognize how little we get as a percentage vs the superintelligent agent. As a more specific example, humans get all of the Milky Galaxy while the superintelligent agents gets the power tower of remaining galaxies leaving us with orders of magnitude more than before but less than 1% of the observable universe. 
Additionally, this analysis will not consider the possibility of humans controlling an AGI that enacts some type of existential catastrophe (my priors lean toward dystopia being the most likely of the three).

## Methodology
The question narrows only onto the control problem. Stuart Russell provides the definition of an advanced agent with a seemingly innocuous, limited goal that could seek out a vast quantity of physical resources—including resources crucial for humans—in order to fulfill that goal as effectively as possible.

### Who?
If AGI is possible earlier, it'll be backed by large nation states (USA, China etc.) if they're able to prioritize budgets.
If AGI is possible slightly later, it'll be developed by a large tech/research company (Alphabet [Google Brain, Deepmind, Anthropic], Microsoft [OpenAI] etc.).
And later still, AGI could be either of the above but scaled down versions to include Canada, UK and various EU countries [Marco Polo](https://macropolo.org/digital-projects/the-global-ai-talent-tracker/) or today's winners in narrow AI (Cohere, Fathom etc.)

### How?
Given the various rankings of the "who are the players", I will build a model 

### When?
