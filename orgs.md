---
layout: page
title: Major AI Organizations
---

# Major AI Organizations
_Last updated March 2023_

## Predicting first AGI Creator
1. [30% likelihood] USA Government
   - My personal view is that once weak AGI is developed, the US Government will mobilize quickly to leverage mankind's last invention.
   - As far as I know, there's not publically available information on this scenario. This opinion is based solely on the belief that "All USA major decision makers (politicians, DoD Generals etc.) want the USA to stay the global superpower."
2. [25% likelihood] Google
   - Subsideraries include Deepmind, Anthropic, & Google Brain.
3. [20% likelihood] China
4. [15% likelihood] Microsoft
   - Major funder for OpenAI
5. [10% likelihood] Some other State or non-state actor

### USA Government - Technology Emphasis
- Intelligence Advanced Research Projects Activity (IARPA)
- National Science and Technology Council (NSTC)
- TODO

### Companies / Nonprofits
- [Alignment Research Center](https://alignmentresearchcenter.org/)
- [Anthropic](https://www.anthropic.com/)
   - A public benefit corporation founded in 2021 by a group of former OpenAI researchers, described as “an AI safety and research company that’s working to build reliable, interpretable, and steerable AI systems.”
   - A broad range of interests at this point - watch this space!
- [AI Impacts](https://aiimpacts.org/)
- [AI Objectives Institute](https://ai.objectives.institute/)
- [Center for Security and Emerging Technology (CSET)](https://cset.georgetown.edu/)
- [Center for AI Safety (CAIS)](https://www.safe.ai/)
- [Center on Long-Term Risk (CLR)](https://longtermrisk.org/)
- [Centre for the Governance of AI (GovAI)](https://governance.ai/)
   - A new spin-out of the Future of Humanity Institute (FHI, see below). “We are building a global research community, dedicated to helping humanity navigate the transition to a world with advanced AI”
- [Cooperative AI Foundation](https://www.cooperativeai.com/home)
- [DeepMind](https://deepmind.com/)
   - A deep learning company aiming to “build safe AI systems, ... [solve] intelligence, advance science and benefit humanity”. Founded in 2010 and bought by Alphabet Inc. (Google’s parent company) in 2014. 
   - They have demonstrated the power of deep reinforcement learning to beat human experts in games (AlphaGo, AlphaZero, etc), and are starting to make inroads in scientific discovery and medicine with e.g. AlphaFold (which tangibly accelerated the field of protein folding).
   - They are largely a technical company with a safety & ethics branch, and their policy research team includes Alan Dafoe, leader of GovAI (see below).
- [Fathom Radiant](https://fathomradiant.co/)
   - Public benefit company building hardware to enable beneficial machine intelligence.
- [Fund for Alignment Research (FAR)](https://far.ai/)
- [Future of Life Institute](https://futureoflife.org/ai-safety-research/)
- [Machine Intelligence Research Institute (MIRI)](https://intelligence.org/)
   - Foundational mathematical research non-profit institute, focused on making superintelligence go well. Leaders in the field of safety on such topics as learned optimisation (inner misalignment), and embedded agency.
- [Nonlinear](https://www.nonlinear.org/)
   - Meta/grant making organisation. “We systematically search for high impact strategies to reduce existential and suffering risks… [and] make it happen. An incubator for interventions.” 
   - [Nonlinear Network](https://nonlinearnetwork.org/) is their funding arm.
- [OpenAI](https://openai.com/)
   - OpenAI was founded “to ensure that artificial general intelligence benefits all of humanity”. They openly commit to building safe AGI and have created some of the most impressive examples of AI systems today, such as the large language model GPT-3 (now open access) and CLIP.
   - Largely a technical company, they have a dedicated “alignment” team and a governance branch called the “AI Futures” team. Originally not-for-profit, it now has a for-profit arm.
- [Ought](https://ought.org/)
   - Working on advanced reasoning in AI systems. Was founded by safety-aligned people.
- [Partnership on AI](https://partnershiponai.org/)
- [Redwood Research](https://www.redwoodresearch.org/)
   - Founded in 2021, its mission is “to align superhuman AI”, which it openly believes is more likely than not to be developed than not this century. They are currently internalising human values into language models, and doing some field-building work such as the ML for Alignment Bootcamp.

### Academic institutions 
- [Center for Human-Compatible AI (CHAI)](https://humancompatible.ai/)
   - A research institute in Berkeley, California. Led by Stuart Russell, long term advocate of the problem of control in AI and author of a best-selling AI textbook. They led the charge on inverse reinforcement learning, and are interested in a wide range of control-focused projects.
- [Centre for the Study of Existential Risk (CSER)](https://www.cser.ac.uk/)
   - A research institute in Cambridge, UK, “dedicated to the study and mitigation of risks that could lead to human extinction or civilisational collapse”. They are largely governance-focused and consider the complex interactions of global risks.
- [Future of Humanity Institute (FHI)](https://www.fhi.ox.ac.uk/)
   - A research institute in Oxford, UK. They collaborate with Oxford DPhil students - Oxford’s name for PhD students - and house academics. Their interests are broad, spanning governance and alignment, and “include idealized reasoning, the incentives of AI systems, and the limitations of value learning.”
- [David Krueger’s](https://www.davidscottkrueger.com/) Research Group (University of Cambridge)
   - Set up in 2021. Krueger is interested in many alignment-related topics, and is an accomplished ML researcher. 
- [Dylan Hadfield-Menell](https://people.csail.mit.edu/dhm/)

### Major Funding bodies
- [Effective Altruism Funds (EA Funds)](https://funds.effectivealtruism.org/)
   - [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future)
- [Future Funding List](https://www.futurefundinglist.com/): Updated table with filters for "Funding for long-term-oriented people and projects"
- Founder's Pledge
- Longview Philanthropy
- [Open Philanthropy (Open Phil)](https://www.openphilanthropy.org/)
   - [Independent Researcher](https://www.openphilanthropy.org/early-career-funding-for-individuals-interested-in-improving-the-long-term-future/)
   - [Century Fellow](https://www.openphilanthropy.org/century-fellowship/)
- Survival and Flourishing Funds ([SFP](http://survivalandflourishing.org/) and [SFF](https://survivalandflourishing.fund/)) 

### Other organizations (mix of safety & capabilities)
- [Aligned AI](https://www.aligned-ai.com/)
- [ALTER](https://alter.org.il/)
- [Arkrose](https://arkose.org/)
- [ARENA](https://www.arena.education/)
- Apollo Research (Marius Hobbhahn)
- [AI Incidents](https://incidentdatabase.ai/about/)
- [AI Safety Support (AISS)](https://www.aisafetysupport.org/home)
   - Runs [AI Safety Camp](https://aisafety.camp/) and provides career advice & community building.
- [Allen Institute for AI](https://allenai.org/)
- [Aleph Alpha](https://aleph-alpha.de/)
- [Berkeley Existential Risk Initiative (BERI)](https://existence.org/)
- [Cohere](https://cohere.ai/)
- [Conjecture](https://www.conjecture.dev/)
- [Convergence](https://www.convergenceanalysis.org/)
- [EleutherAI](https://www.eleuther.ai/)
- [EnculturedAI](https://www.encultured.ai/)
- [Forefront](https://www.forefront.ai/)
- [Global Catastrophic Risk Institute (GCRI)](http://gcrinstitute.org/)
- [Global Priorities Institute](https://globalprioritiesinstitute.org/)
- [Google Brain](https://research.google/teams/brain/)
- [Hugging Face](https://huggingface.co/)
- [Institute for Artificial Intelligence and Fundamental Interactions (IAIFI)](https://iaifi.org/)
   - Pursues massive decentralised research projects, mostly replicating state of the art large ML systems (e.g. their GPT-J). Their mission is to make the benefits of advanced AI open source and in individuals’ hands. They have access to compute from CoreWeave.
- [Leverhulme Centre for the Future of Intelligence (CFI)](http://lcfi.ac.uk/)
- [Median Group](http://mediangroup.org/)
- [Obelisk](https://astera.org/obelisk/)
- [Preamble](https://www.preamble.com/about-us)
- [Render](https://rendertoken.com/)
- [Stanford Existential Risk Institute](https://seri.stanford.edu/)
- [Stanford Human-Centered Artificial Intelligence (HAI)](https://hai.stanford.edu/)
- [Vast](https://vast.ai/)
- [Weights & Biases](https://wandb.ai/site)

## Misc. Resources
_The above list is mainly from [BlueDot's AI Safety Resources page](https://www.agisafetyfundamentals.com/resources) with added commentary by [Jamie Bernardi.](https://jamiebernardi.com/)
The lists are in alphabetical order with the organizations full name (not using acroymns for ordering) where I've added several others. I'll likely re-organize these at a later time._
- [Bibliography Database](https://www.lesswrong.com/posts/4DegbDJJiMX2b3EKm/tai-safety-bibliographic-database)
- [AI Safety Support](https://www.aisafetysupport.org/resources/lots-of-links)
- [Awesome AI Alignment](https://github.com/dit7ya/awesome-ai-alignment)
- [Marco Polo - AI Talent Tracker (Project)](https://macropolo.org/digital-projects/the-global-ai-talent-tracker/)
- [Estimating AI Safety Employees](https://www.lesswrong.com/posts/mC3oeq62DWeqxiNBx/estimating-the-current-and-future-number-of-ai-safety)
- [Victoria Krakovna Misc. resources](https://vkrakovna.wordpress.com/ai-safety-resources/#communities)
- [Gdoc with various resources](https://docs.google.com/document/d/1z0QoDEu6WmubZSqh7ejgGtBjTR0i0SfLwyNgdcD9kBc/edit#heading=h.b7aa6ksd98wr)
- Non-public Google Sheet of AI Safety Researchers (message me if you'd like the link as the author asked for it to not be widely shared yet)