# AI Alignment Organizations

## Companies / nonprofits
- [AI Objectives Institute](https://ai.objectives.institute/)
- Alignment Research Center
- Anthropic 
   - A public benefit corporation founded in 2021 by a group of former OpenAI researchers, described as “an AI safety and research company that’s working to build reliable, interpretable, and steerable AI systems.”
   - A broad range of interests at this point - watch this space!
- DeepMind
   - A deep learning company aiming to “build safe AI systems, ... [solve] intelligence, advance science and benefit humanity”. Founded in 2010 and bought by Alphabet Inc. (Google’s parent company) in 2014. 
   - They have demonstrated the power of deep reinforcement learning to beat human experts in games (AlphaGo, AlphaZero, etc), and are starting to make inroads in scientific discovery and medicine with e.g. AlphaFold (which tangibly accelerated the field of protein folding).
   - They are largely a technical company with a safety & ethics branch, and their policy research team includes Alan Dafoe, leader of GovAI (see below).
- OpenAI
   - OpenAI was founded “to ensure that artificial general intelligence benefits all of humanity”. They openly commit to building safe AGI and have created some of the most impressive examples of AI systems today, such as the large language model GPT-3 (now open access) and CLIP.
   - Largely a technical company, they have a dedicated “alignment” team and a governance branch called the “AI Futures” team. Originally not-for-profit, it now has a for-profit arm.

- Machine Intelligence Research Institute (MIRI) 
   - Foundational mathematical research non-profit institute, focused on making superintelligence go well. Leaders in the field of safety on such topics as learned optimisation (inner misalignment), and embedded agency.
- Nonlinear
   - Meta/grant making organisation. “We systematically search for high impact strategies to reduce existential and suffering risks… [and] make it happen. An incubator for interventions.” They are currently in a research phase, and I don’t see many opportunities to obtain funding yet.
- Redwood Research
   - Founded in 2021, its mission is “to align superhuman AI”, which it openly believes is more likely than not to be developed than not this century. They are currently internalising human values into language models, and doing some field-building work such as the ML for Alignment Bootcamp.
- Center for Security and Emerging Technology (CSET)
- Cooperative AI Foundation
- Center on Long-Term Risk (CLR)
- Centre for the Governance of AI (GovAI)
   - A new spin-out of the Future of Humanity Institute (FHI, see below). “We are building a global research community, dedicated to helping humanity navigate the transition to a world with advanced AI”
- AI Impacts

## Academic institutions 

- Center for Human-Compatible AI (CHAI)
   - A research institute in Berkeley, California. Led by Stuart Russell, long term advocate of the problem of control in AI and author of a best-selling AI textbook. They led the charge on inverse reinforcement learning, and are interested in a wide range of control-focused projects.
- Future of Humanity Institute (FHI)
   A research institute in Oxford, UK. They collaborate with Oxford DPhil students - Oxford’s name for PhD students - and house academics. Their interests are broad, spanning governance and alignment, and “include idealized reasoning, the incentives of AI systems, and the limitations of value learning.”
- Centre for the Study of Existential Risk (CSER)
   - A research institute in Cambridge, UK, “dedicated to the study and mitigation of risks that could lead to human extinction or civilisational collapse”. They are largely governance-focused and consider the complex interactions of global risks.
- David Krueger’s Research Group (University of Cambridge)
   - Set up in 2021. Krueger is interested in many alignment-related topics, and is an accomplished ML researcher. 
- 

## Funding bodies
- Open Philanthropy 
- Effective Altruism Funds 
- Survival and Flourishing Funds (SAF and SFF) 

## Other institutes (not necessarily focused on safety)
- Allen Institute for AI
- Aleph Alpha
- CohereAI
- EleutherAI
   - Pursues massive decentralised research projects, mostly replicating state of the art large ML systems (e.g. their GPT-J). As I understand it, their mission is to make the benefits of advanced AI open source and in individuals’ hands. They have access to compute from CoreWeave. 
- Google Brain
- Institute for Artificial Intelligence and Fundamental Interactions (IAIFI)
- Ought
   - Working on advanced reasoning in AI systems. Was founded by safety-aligned people.
- Preamble
- Stanford Human-Centered Artificial Intelligence (HAI)

# Misc. Resources
_The above list is mainly from the [BlueDot AGI Resource page](https://www.agisafetyfundamentals.com/resources)
in alphabetical order where I've added a few others. I'll likely re-organize these at a later time._

- [AI Safety Support](https://www.aisafetysupport.org/resources/lots-of-links)
- [Awesome AI Alignment](https://github.com/dit7ya/awesome-ai-alignment)